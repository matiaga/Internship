{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c99b986f",
   "metadata": {},
   "source": [
    "# Web Scraping - Asignment 4\n",
    "By Mónica Atiaga\n",
    "\n",
    "Batch - DSNB1222\n",
    "\n",
    "Instructions:\n",
    "* Read all the problem statements, notes carefully and scrape the required data using any web scraping tool of your choice. \n",
    "* You have to handle commonly occurring EXCEPTIONS by using exception handling programing. To get information about selenium Exceptions. You may visit following links:\n",
    "\n",
    "1. https://selenium-python.readthedocs.io/api.html\n",
    "2. https://www.guru99.com/exception-handling-selenium.html\n",
    "3. https://stackoverflow.com/questions/38022658/selenium-python-handling-no-such-element\u0002exception/38023345\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2abf41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries \n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException \n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import regex as re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a0f055",
   "metadata": {},
   "source": [
    "#### My Custom Functions (at the end section)\n",
    "\n",
    "\n",
    "* get_attribute_el(webelement, attribute)\n",
    "* text_el(webelement)\n",
    "* go_next_page(driver, xpath)\n",
    "* load_items_by(driver, n=None, **kwargs)\n",
    "* remove_popup(xpath)\n",
    "* scrape_item_detail(driver, not_found_value = '-', **kwargs)\n",
    "* scrape_data_by_urls(urls, attributes_det)\n",
    "* web_scraping_table(xpath_table, init_data=1, footer=False)\n",
    "\n",
    "All functions used are my own. As we have progressed in the assignments, I have been improving the code so that they are more general and can be applied in different scenarios. I decided to do it this way to improve and progress my knowledge of python.\n",
    "By Monica Atiaga "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d54177d",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f306ff5",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia. \n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos \n",
    "\n",
    "    You need to find following details: \n",
    "        A) Rank \n",
    "        B) Name \n",
    "        C) Artist \n",
    "        D) Upload date \n",
    "        E) Views "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e457848a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['No.', 'Video name', 'Uploader', 'Views (billions)', 'Publication date', 'Note']\n",
      "Recovered 30 records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Views (billions)</th>\n",
       "      <th>Upload date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>\"Baby Shark Dance\"[6]</td>\n",
       "      <td>Pinkfong Baby Shark - children's songs</td>\n",
       "      <td>13.18</td>\n",
       "      <td>June 17, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>\"Despacito\"[9]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>8.23</td>\n",
       "      <td>January 12, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[16]</td>\n",
       "      <td>LooLoo Kids - nursery rhymes</td>\n",
       "      <td>6.76</td>\n",
       "      <td>October 8, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>\"Bath Song\"[17]</td>\n",
       "      <td>Cocomelon - nursery rhymes</td>\n",
       "      <td>6.33</td>\n",
       "      <td>May 2, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>\"Shape of You\"[18]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>6.05</td>\n",
       "      <td>January 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>\"See You Again\"[21]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>5.98</td>\n",
       "      <td>April 6, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>\"Wheels on the Bus\"[26]</td>\n",
       "      <td>Cocomelon - nursery rhymes</td>\n",
       "      <td>5.46</td>\n",
       "      <td>May 24, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>\"Phonics Song with Two Words\"[27]</td>\n",
       "      <td>ChuChu TV - children's songs</td>\n",
       "      <td>5.42</td>\n",
       "      <td>March 6, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>\"Uptown Funk\"[28]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>5.00</td>\n",
       "      <td>November 19, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[29]</td>\n",
       "      <td>Miroshka TV - children's songs</td>\n",
       "      <td>4.94</td>\n",
       "      <td>February 27, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>\"Gangnam Style\"[30]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>4.86</td>\n",
       "      <td>July 15, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[35]</td>\n",
       "      <td>Get Movies - children's songs</td>\n",
       "      <td>4.55</td>\n",
       "      <td>January 31, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>\"Dame Tu Cosita\"[36]</td>\n",
       "      <td>El Chombo</td>\n",
       "      <td>4.41</td>\n",
       "      <td>April 5, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>\"Axel F\"[37]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>4.00</td>\n",
       "      <td>June 16, 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>\"Sugar\"[38]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>3.91</td>\n",
       "      <td>January 14, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>\"Roar\"[39]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>3.84</td>\n",
       "      <td>September 5, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>\"Counting Stars\"[40]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>3.84</td>\n",
       "      <td>May 31, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>\"Baa Baa Black Sheep\"[41]</td>\n",
       "      <td>Cocomelon - nursery rhymes</td>\n",
       "      <td>3.73</td>\n",
       "      <td>June 25, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>\"Sorry\"[42]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>3.69</td>\n",
       "      <td>October 22, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[43]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>3.68</td>\n",
       "      <td>June 4, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>\"Thinking Out Loud\"[44]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.63</td>\n",
       "      <td>October 7, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>\"Lakdi Ki Kathi\"[45]</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>3.63</td>\n",
       "      <td>June 14, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>\"Dark Horse\"[46]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>3.56</td>\n",
       "      <td>February 20, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>\"Perfect\"[47]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.51</td>\n",
       "      <td>November 9, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>\"Faded\"[48]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>3.49</td>\n",
       "      <td>December 3, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>\"Let Her Go\"[49]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>3.48</td>\n",
       "      <td>July 25, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>\"Humpty the train on a fruits ride\"[50]</td>\n",
       "      <td>Kiddiestv Hindi - children's songs</td>\n",
       "      <td>3.51</td>\n",
       "      <td>January 26, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>\"Girls Like You\"[51]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>3.45</td>\n",
       "      <td>May 31, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>\"Bailando\"[52]</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>3.43</td>\n",
       "      <td>April 11, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>\"Lean On\"[53]</td>\n",
       "      <td>Major Lazer</td>\n",
       "      <td>3.43</td>\n",
       "      <td>March 22, 2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                             Name  \\\n",
       "0    1.                            \"Baby Shark Dance\"[6]   \n",
       "1    2.                                   \"Despacito\"[9]   \n",
       "2    3.                       \"Johny Johny Yes Papa\"[16]   \n",
       "3    4.                                  \"Bath Song\"[17]   \n",
       "4    5.                               \"Shape of You\"[18]   \n",
       "5    6.                              \"See You Again\"[21]   \n",
       "6    7.                          \"Wheels on the Bus\"[26]   \n",
       "7    8.                \"Phonics Song with Two Words\"[27]   \n",
       "8    9.                                \"Uptown Funk\"[28]   \n",
       "9   10.  \"Learning Colors – Colorful Eggs on a Farm\"[29]   \n",
       "10  11.                              \"Gangnam Style\"[30]   \n",
       "11  12.   \"Masha and the Bear – Recipe for Disaster\"[35]   \n",
       "12  13.                             \"Dame Tu Cosita\"[36]   \n",
       "13  14.                                     \"Axel F\"[37]   \n",
       "14  15.                                      \"Sugar\"[38]   \n",
       "15  16.                                       \"Roar\"[39]   \n",
       "16  17.                             \"Counting Stars\"[40]   \n",
       "17  18.                        \"Baa Baa Black Sheep\"[41]   \n",
       "18  19.                                      \"Sorry\"[42]   \n",
       "19  20.           \"Waka Waka (This Time for Africa)\"[43]   \n",
       "20  21.                          \"Thinking Out Loud\"[44]   \n",
       "21  22.                             \"Lakdi Ki Kathi\"[45]   \n",
       "22  23.                                 \"Dark Horse\"[46]   \n",
       "23  24.                                    \"Perfect\"[47]   \n",
       "24  25.                                      \"Faded\"[48]   \n",
       "25  26.                                 \"Let Her Go\"[49]   \n",
       "26  27.          \"Humpty the train on a fruits ride\"[50]   \n",
       "27  28.                             \"Girls Like You\"[51]   \n",
       "28  29.                                   \"Bailando\"[52]   \n",
       "29  30.                                    \"Lean On\"[53]   \n",
       "\n",
       "                                    Artist Views (billions)        Upload date  \n",
       "0   Pinkfong Baby Shark - children's songs            13.18      June 17, 2016  \n",
       "1                               Luis Fonsi             8.23   January 12, 2017  \n",
       "2             LooLoo Kids - nursery rhymes             6.76    October 8, 2016  \n",
       "3               Cocomelon - nursery rhymes             6.33        May 2, 2018  \n",
       "4                               Ed Sheeran             6.05   January 30, 2017  \n",
       "5                              Wiz Khalifa             5.98      April 6, 2015  \n",
       "6               Cocomelon - nursery rhymes             5.46       May 24, 2018  \n",
       "7             ChuChu TV - children's songs             5.42      March 6, 2014  \n",
       "8                              Mark Ronson             5.00  November 19, 2014  \n",
       "9           Miroshka TV - children's songs             4.94  February 27, 2018  \n",
       "10                                     Psy             4.86      July 15, 2012  \n",
       "11           Get Movies - children's songs             4.55   January 31, 2012  \n",
       "12                               El Chombo             4.41      April 5, 2018  \n",
       "13                              Crazy Frog             4.00      June 16, 2009  \n",
       "14                                Maroon 5             3.91   January 14, 2015  \n",
       "15                              Katy Perry             3.84  September 5, 2013  \n",
       "16                             OneRepublic             3.84       May 31, 2013  \n",
       "17              Cocomelon - nursery rhymes             3.73      June 25, 2018  \n",
       "18                           Justin Bieber             3.69   October 22, 2015  \n",
       "19                                 Shakira             3.68       June 4, 2010  \n",
       "20                              Ed Sheeran             3.63    October 7, 2014  \n",
       "21                            Jingle Toons             3.63      June 14, 2018  \n",
       "22                              Katy Perry             3.56  February 20, 2014  \n",
       "23                              Ed Sheeran             3.51   November 9, 2017  \n",
       "24                             Alan Walker             3.49   December 3, 2015  \n",
       "25                               Passenger             3.48      July 25, 2012  \n",
       "26      Kiddiestv Hindi - children's songs             3.51   January 26, 2018  \n",
       "27                                Maroon 5             3.45       May 31, 2018  \n",
       "28                        Enrique Iglesias             3.43     April 11, 2014  \n",
       "29                             Major Lazer             3.43     March 22, 2015  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Open the Driver \n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "driver.get(url) \n",
    "\n",
    "wait_time = 2\n",
    "\n",
    "# Waiting for the data\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "# Scraping the table - function at the end section\n",
    "xpath_table = '//table[@class=\"wikitable sortable jquery-tablesorter\"]'\n",
    "df, columns = web_scraping_table(xpath_table, footer=True)\n",
    "\n",
    "# cleaning driver\n",
    "driver.quit()\n",
    "\n",
    "# Setting up the dataframe\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.drop(['Note'], axis=1, inplace=True)\n",
    "df.columns = ['Rank', 'Name', 'Artist', 'Views (billions)', 'Upload date']\n",
    "\n",
    "# Saving the data frame\n",
    "df.to_csv('datasets/q1_most_viewed_videos.csv')\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a586eb",
   "metadata": {},
   "source": [
    "2. Scrape the details team India’s international fixtures from bcci.tv. \n",
    "\n",
    "    Url = https://www.bcci.tv/.\n",
    "    \n",
    "    You need to find following details: \n",
    "        A) Match title (I.e. 1 ODI) \n",
    "        B) Series \n",
    "        C) Place \n",
    "        D) Date \n",
    "        E) Time \n",
    "    Note: - From bcci.tv home page you have reach to the international fixture page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fb3bfaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reaching the international fixtures...\n",
      "Pop-up removed\n",
      " Going to More Fixtures\n",
      " Going to More Fixtures\n",
      " Going to More Fixtures\n",
      " Going to More Fixtures\n",
      "NO More Fixtures: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//button[contains(@class,\"match-btn\")]\"}\n",
      "  (Session info: chrome=115.0.5790.171); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Match title retrieved and included: 38\n",
      "Series retrieved and included: 38\n",
      "Place retrieved and included: 38\n",
      "Date retrieved and included: 38\n",
      "Time retrieved and included: 38\n"
     ]
    }
   ],
   "source": [
    "# Open the Driver \n",
    "url = \"https://www.bcci.tv/\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url) \n",
    "\n",
    "wait_time = 3\n",
    "time.sleep(wait_time)\n",
    "\n",
    "# Reaching the international fixture page\n",
    "xpath_link = '//a[@data-element_text=\"INTERNATIONAL\"]'\n",
    "link = driver.find_element(By.XPATH, xpath_link)\n",
    "link.click()\n",
    "print(\"Reaching the international fixtures...\")\n",
    "\n",
    "time.sleep(wait_time)\n",
    "\n",
    "# Removing the cookies pop up\n",
    "remove_popup('//*[@class=\"cookie__accept btn btn-primary\"]')\n",
    "\n",
    "# Loading More Fixtures until there is no more\n",
    "while(True):\n",
    "    try:\n",
    "        wait_time = 5\n",
    "        time.sleep(wait_time)\n",
    "        xpath_next = '//button[contains(@class,\"match-btn\")]'\n",
    "        go_next_page(driver, xpath_next)\n",
    "    except Exception as e:\n",
    "        print(f\"NO More Fixtures: {e.msg}\")\n",
    "        break\n",
    "        \n",
    "# Scraping the data -function at the end section\n",
    "fn_text = lambda web: text_el(web)\n",
    "attributes = {'Match title': [By.XPATH, '//*[contains(@class,\"matchOrderText\")]', fn_text, False ],\n",
    "              'Series': [By.XPATH, '//*[@class=\"match-tournament-name ng-binding\"]', fn_text, False ],\n",
    "              'Place': [By.XPATH, '//*[@class=\"match-place ng-scope\"]', fn_text, False ],\n",
    "              'Date': [By.XPATH, '//*[@class=\"match-dates ng-binding\"]', fn_text, False], \n",
    "              'Time': [By.XPATH, '//*[@class=\"match-time no-margin ng-binding\"]', fn_text, False]}\n",
    "df = load_items_by(driver, None, **attributes)\n",
    "\n",
    "# Cleaning the driver\n",
    "driver.quit()\n",
    "\n",
    "# Cleaning the data frame\n",
    "pattern1 = r'-(.*)$'\n",
    "df['Place'] = df['Place'].str.extract(pattern1)\n",
    "pattern2 = r'(.*) -'\n",
    "df['Match title'] = df['Match title'].str.extract(pattern2)\n",
    "\n",
    "# Saving the data frame\n",
    "df.to_csv(\"datasets/q2_international_fixtures.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "842fa0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match title</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st T20I</td>\n",
       "      <td>INDIA TOUR OF IRELAND 2023</td>\n",
       "      <td>The Village, Dublin</td>\n",
       "      <td>18 AUG 2023</td>\n",
       "      <td>9:00 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2nd T20I</td>\n",
       "      <td>INDIA TOUR OF IRELAND 2023</td>\n",
       "      <td>The Village, Dublin</td>\n",
       "      <td>20 AUG 2023</td>\n",
       "      <td>9:00 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>INDIA TOUR OF IRELAND 2023</td>\n",
       "      <td>The Village, Dublin</td>\n",
       "      <td>23 AUG 2023</td>\n",
       "      <td>9:00 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1st ODI</td>\n",
       "      <td>ASIA CUP 2023</td>\n",
       "      <td>Pallekele International Cricket Stadium, Pall...</td>\n",
       "      <td>1 SEP 2023</td>\n",
       "      <td>11:30 PM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2nd ODI</td>\n",
       "      <td>ASIA CUP 2023</td>\n",
       "      <td>Pallekele International Cricket Stadium, Pall...</td>\n",
       "      <td>3 SEP 2023</td>\n",
       "      <td>11:30 PM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1st ODI</td>\n",
       "      <td>AUSTRALIA TOUR OF INDIA 2023-24</td>\n",
       "      <td>Punjab Cricket Association IS Bindra Stadium,...</td>\n",
       "      <td>22 SEP 2023</td>\n",
       "      <td>3:00 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2nd ODI</td>\n",
       "      <td>AUSTRALIA TOUR OF INDIA 2023-24</td>\n",
       "      <td>Holkar Cricket Stadium, Indore</td>\n",
       "      <td>24 SEP 2023</td>\n",
       "      <td>3:00 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3rd ODI</td>\n",
       "      <td>AUSTRALIA TOUR OF INDIA 2023-24</td>\n",
       "      <td>Saurashtra Cricket Association Stadium, Rajkot</td>\n",
       "      <td>27 SEP 2023</td>\n",
       "      <td>3:00 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1st ODI</td>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>MA Chidambaram Stadium, Chennai</td>\n",
       "      <td>8 OCT 2023</td>\n",
       "      <td>3:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2nd ODI</td>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Arun Jaitley Stadium, Delhi</td>\n",
       "      <td>11 OCT 2023</td>\n",
       "      <td>3:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3rd ODI</td>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Narendra Modi Stadium, Ahmedabad</td>\n",
       "      <td>14 OCT 2023</td>\n",
       "      <td>3:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4th ODI</td>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Maharashtra Cricket Association Stadium, Pune</td>\n",
       "      <td>19 OCT 2023</td>\n",
       "      <td>3:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5th ODI</td>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Himachal Pradesh Cricket Association Stadium,...</td>\n",
       "      <td>22 OCT 2023</td>\n",
       "      <td>3:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6th ODI</td>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Bharat Ratna Shri Atal Bihari Vajpayee Ekana ...</td>\n",
       "      <td>29 OCT 2023</td>\n",
       "      <td>3:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7th ODI</td>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Wankhede Stadium, Mumbai</td>\n",
       "      <td>2 NOV 2023</td>\n",
       "      <td>3:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8th ODI</td>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>Eden Gardens, Kolkata</td>\n",
       "      <td>5 NOV 2023</td>\n",
       "      <td>3:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9th ODI</td>\n",
       "      <td>ICC MENS WORLD CUP 2023</td>\n",
       "      <td>M Chinnaswamy Stadium, Bengaluru</td>\n",
       "      <td>12 NOV 2023</td>\n",
       "      <td>3:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1st T20I</td>\n",
       "      <td>AUSTRALIA TOUR OF INDIA 2023-24</td>\n",
       "      <td>Dr YS Rajasekhara Reddy ACA-VDCA Cricket Stad...</td>\n",
       "      <td>23 NOV 2023</td>\n",
       "      <td>8:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2nd T20I</td>\n",
       "      <td>AUSTRALIA TOUR OF INDIA 2023-24</td>\n",
       "      <td>Greenfield International Stadium, Thiruvanant...</td>\n",
       "      <td>26 NOV 2023</td>\n",
       "      <td>8:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>AUSTRALIA TOUR OF INDIA 2023-24</td>\n",
       "      <td>Barsapara Cricket Stadium, Guwahati</td>\n",
       "      <td>28 NOV 2023</td>\n",
       "      <td>8:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4th T20I</td>\n",
       "      <td>AUSTRALIA TOUR OF INDIA 2023-24</td>\n",
       "      <td>Vidarbha Cricket Association Stadium, Nagpur</td>\n",
       "      <td>1 DEC 2023</td>\n",
       "      <td>8:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5th T20I</td>\n",
       "      <td>AUSTRALIA TOUR OF INDIA 2023-24</td>\n",
       "      <td>Rajiv Gandhi International Stadium, Hyderabad</td>\n",
       "      <td>3 DEC 2023</td>\n",
       "      <td>8:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1st T20I</td>\n",
       "      <td>INDIA TOUR OF SOUTH AFRICA 2023-24</td>\n",
       "      <td>Kingsmead, Durban</td>\n",
       "      <td>10 DEC 2023</td>\n",
       "      <td>11:00 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2nd T20I</td>\n",
       "      <td>INDIA TOUR OF SOUTH AFRICA 2023-24</td>\n",
       "      <td>St George's Park, Gqeberha</td>\n",
       "      <td>12 DEC 2023</td>\n",
       "      <td>11:00 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>INDIA TOUR OF SOUTH AFRICA 2023-24</td>\n",
       "      <td>The Wanderers Stadium, Johannesburg</td>\n",
       "      <td>14 DEC 2023</td>\n",
       "      <td>11:00 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1st ODI</td>\n",
       "      <td>INDIA TOUR OF SOUTH AFRICA 2023-24</td>\n",
       "      <td>Johannesburg</td>\n",
       "      <td>17 DEC 2023</td>\n",
       "      <td>3:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2nd ODI</td>\n",
       "      <td>INDIA TOUR OF SOUTH AFRICA 2023-24</td>\n",
       "      <td>St George's Park, Gqeberha</td>\n",
       "      <td>19 DEC 2023</td>\n",
       "      <td>3:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3rd ODI</td>\n",
       "      <td>INDIA TOUR OF SOUTH AFRICA 2023-24</td>\n",
       "      <td>Boland Park, Paarl</td>\n",
       "      <td>21 DEC 2023</td>\n",
       "      <td>3:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1st Test</td>\n",
       "      <td>INDIA TOUR OF SOUTH AFRICA 2023-24</td>\n",
       "      <td>SuperSport Park, Centurion</td>\n",
       "      <td>26 DEC 2023</td>\n",
       "      <td>3:00 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2nd Test</td>\n",
       "      <td>INDIA TOUR OF SOUTH AFRICA 2023-24</td>\n",
       "      <td>Newlands, Cape Town</td>\n",
       "      <td>3 JAN 2024</td>\n",
       "      <td>3:00 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1st T20I</td>\n",
       "      <td>AFGHANISTAN TOUR OF INDIA 2023-24</td>\n",
       "      <td>Punjab Cricket Association IS Bindra Stadium,...</td>\n",
       "      <td>11 JAN 2024</td>\n",
       "      <td>8:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2nd T20I</td>\n",
       "      <td>AFGHANISTAN TOUR OF INDIA 2023-24</td>\n",
       "      <td>Holkar Cricket Stadium, Indore</td>\n",
       "      <td>14 JAN 2024</td>\n",
       "      <td>8:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>AFGHANISTAN TOUR OF INDIA 2023-24</td>\n",
       "      <td>M Chinnaswamy Stadium, Bengaluru</td>\n",
       "      <td>17 JAN 2024</td>\n",
       "      <td>8:30 AM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1st Test</td>\n",
       "      <td>ENGLAND TOUR OF INDIA 2023-24</td>\n",
       "      <td>Rajiv Gandhi International Stadium, Hyderabad</td>\n",
       "      <td>24 JAN 2024</td>\n",
       "      <td>11:00 PM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2nd Test</td>\n",
       "      <td>ENGLAND TOUR OF INDIA 2023-24</td>\n",
       "      <td>Dr YS Rajasekhara Reddy ACA-VDCA Cricket Stad...</td>\n",
       "      <td>1 FEB 2024</td>\n",
       "      <td>11:00 PM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3rd Test</td>\n",
       "      <td>ENGLAND TOUR OF INDIA 2023-24</td>\n",
       "      <td>Saurashtra Cricket Association Stadium, Rajkot</td>\n",
       "      <td>14 FEB 2024</td>\n",
       "      <td>11:00 PM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4th Test</td>\n",
       "      <td>ENGLAND TOUR OF INDIA 2023-24</td>\n",
       "      <td>JSCA International Stadium Complex, Ranchi</td>\n",
       "      <td>22 FEB 2024</td>\n",
       "      <td>11:00 PM HDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5th Test</td>\n",
       "      <td>ENGLAND TOUR OF INDIA 2023-24</td>\n",
       "      <td>Himachal Pradesh Cricket Association Stadium,...</td>\n",
       "      <td>6 MAR 2024</td>\n",
       "      <td>11:00 PM HDE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Match title                              Series  \\\n",
       "0     1st T20I          INDIA TOUR OF IRELAND 2023   \n",
       "1     2nd T20I          INDIA TOUR OF IRELAND 2023   \n",
       "2     3rd T20I          INDIA TOUR OF IRELAND 2023   \n",
       "3      1st ODI                       ASIA CUP 2023   \n",
       "4      2nd ODI                       ASIA CUP 2023   \n",
       "5      1st ODI     AUSTRALIA TOUR OF INDIA 2023-24   \n",
       "6      2nd ODI     AUSTRALIA TOUR OF INDIA 2023-24   \n",
       "7      3rd ODI     AUSTRALIA TOUR OF INDIA 2023-24   \n",
       "8      1st ODI             ICC MENS WORLD CUP 2023   \n",
       "9      2nd ODI             ICC MENS WORLD CUP 2023   \n",
       "10     3rd ODI             ICC MENS WORLD CUP 2023   \n",
       "11     4th ODI             ICC MENS WORLD CUP 2023   \n",
       "12     5th ODI             ICC MENS WORLD CUP 2023   \n",
       "13     6th ODI             ICC MENS WORLD CUP 2023   \n",
       "14     7th ODI             ICC MENS WORLD CUP 2023   \n",
       "15     8th ODI             ICC MENS WORLD CUP 2023   \n",
       "16     9th ODI             ICC MENS WORLD CUP 2023   \n",
       "17    1st T20I     AUSTRALIA TOUR OF INDIA 2023-24   \n",
       "18    2nd T20I     AUSTRALIA TOUR OF INDIA 2023-24   \n",
       "19    3rd T20I     AUSTRALIA TOUR OF INDIA 2023-24   \n",
       "20    4th T20I     AUSTRALIA TOUR OF INDIA 2023-24   \n",
       "21    5th T20I     AUSTRALIA TOUR OF INDIA 2023-24   \n",
       "22    1st T20I  INDIA TOUR OF SOUTH AFRICA 2023-24   \n",
       "23    2nd T20I  INDIA TOUR OF SOUTH AFRICA 2023-24   \n",
       "24    3rd T20I  INDIA TOUR OF SOUTH AFRICA 2023-24   \n",
       "25     1st ODI  INDIA TOUR OF SOUTH AFRICA 2023-24   \n",
       "26     2nd ODI  INDIA TOUR OF SOUTH AFRICA 2023-24   \n",
       "27     3rd ODI  INDIA TOUR OF SOUTH AFRICA 2023-24   \n",
       "28    1st Test  INDIA TOUR OF SOUTH AFRICA 2023-24   \n",
       "29    2nd Test  INDIA TOUR OF SOUTH AFRICA 2023-24   \n",
       "30    1st T20I   AFGHANISTAN TOUR OF INDIA 2023-24   \n",
       "31    2nd T20I   AFGHANISTAN TOUR OF INDIA 2023-24   \n",
       "32    3rd T20I   AFGHANISTAN TOUR OF INDIA 2023-24   \n",
       "33    1st Test       ENGLAND TOUR OF INDIA 2023-24   \n",
       "34    2nd Test       ENGLAND TOUR OF INDIA 2023-24   \n",
       "35    3rd Test       ENGLAND TOUR OF INDIA 2023-24   \n",
       "36    4th Test       ENGLAND TOUR OF INDIA 2023-24   \n",
       "37    5th Test       ENGLAND TOUR OF INDIA 2023-24   \n",
       "\n",
       "                                                Place         Date  \\\n",
       "0                                 The Village, Dublin  18 AUG 2023   \n",
       "1                                 The Village, Dublin  20 AUG 2023   \n",
       "2                                 The Village, Dublin  23 AUG 2023   \n",
       "3    Pallekele International Cricket Stadium, Pall...   1 SEP 2023   \n",
       "4    Pallekele International Cricket Stadium, Pall...   3 SEP 2023   \n",
       "5    Punjab Cricket Association IS Bindra Stadium,...  22 SEP 2023   \n",
       "6                      Holkar Cricket Stadium, Indore  24 SEP 2023   \n",
       "7      Saurashtra Cricket Association Stadium, Rajkot  27 SEP 2023   \n",
       "8                     MA Chidambaram Stadium, Chennai   8 OCT 2023   \n",
       "9                         Arun Jaitley Stadium, Delhi  11 OCT 2023   \n",
       "10                   Narendra Modi Stadium, Ahmedabad  14 OCT 2023   \n",
       "11      Maharashtra Cricket Association Stadium, Pune  19 OCT 2023   \n",
       "12   Himachal Pradesh Cricket Association Stadium,...  22 OCT 2023   \n",
       "13   Bharat Ratna Shri Atal Bihari Vajpayee Ekana ...  29 OCT 2023   \n",
       "14                           Wankhede Stadium, Mumbai   2 NOV 2023   \n",
       "15                              Eden Gardens, Kolkata   5 NOV 2023   \n",
       "16                   M Chinnaswamy Stadium, Bengaluru  12 NOV 2023   \n",
       "17   Dr YS Rajasekhara Reddy ACA-VDCA Cricket Stad...  23 NOV 2023   \n",
       "18   Greenfield International Stadium, Thiruvanant...  26 NOV 2023   \n",
       "19                Barsapara Cricket Stadium, Guwahati  28 NOV 2023   \n",
       "20       Vidarbha Cricket Association Stadium, Nagpur   1 DEC 2023   \n",
       "21      Rajiv Gandhi International Stadium, Hyderabad   3 DEC 2023   \n",
       "22                                  Kingsmead, Durban  10 DEC 2023   \n",
       "23                         St George's Park, Gqeberha  12 DEC 2023   \n",
       "24                The Wanderers Stadium, Johannesburg  14 DEC 2023   \n",
       "25                                       Johannesburg  17 DEC 2023   \n",
       "26                         St George's Park, Gqeberha  19 DEC 2023   \n",
       "27                                 Boland Park, Paarl  21 DEC 2023   \n",
       "28                         SuperSport Park, Centurion  26 DEC 2023   \n",
       "29                                Newlands, Cape Town   3 JAN 2024   \n",
       "30   Punjab Cricket Association IS Bindra Stadium,...  11 JAN 2024   \n",
       "31                     Holkar Cricket Stadium, Indore  14 JAN 2024   \n",
       "32                   M Chinnaswamy Stadium, Bengaluru  17 JAN 2024   \n",
       "33      Rajiv Gandhi International Stadium, Hyderabad  24 JAN 2024   \n",
       "34   Dr YS Rajasekhara Reddy ACA-VDCA Cricket Stad...   1 FEB 2024   \n",
       "35     Saurashtra Cricket Association Stadium, Rajkot  14 FEB 2024   \n",
       "36         JSCA International Stadium Complex, Ranchi  22 FEB 2024   \n",
       "37   Himachal Pradesh Cricket Association Stadium,...   6 MAR 2024   \n",
       "\n",
       "            Time  \n",
       "0    9:00 AM HDE  \n",
       "1    9:00 AM HDE  \n",
       "2    9:00 AM HDE  \n",
       "3   11:30 PM HDE  \n",
       "4   11:30 PM HDE  \n",
       "5    3:00 AM HDE  \n",
       "6    3:00 AM HDE  \n",
       "7    3:00 AM HDE  \n",
       "8    3:30 AM HDE  \n",
       "9    3:30 AM HDE  \n",
       "10   3:30 AM HDE  \n",
       "11   3:30 AM HDE  \n",
       "12   3:30 AM HDE  \n",
       "13   3:30 AM HDE  \n",
       "14   3:30 AM HDE  \n",
       "15   3:30 AM HDE  \n",
       "16   3:30 AM HDE  \n",
       "17   8:30 AM HDE  \n",
       "18   8:30 AM HDE  \n",
       "19   8:30 AM HDE  \n",
       "20   8:30 AM HDE  \n",
       "21   8:30 AM HDE  \n",
       "22  11:00 AM HDE  \n",
       "23  11:00 AM HDE  \n",
       "24  11:00 AM HDE  \n",
       "25   3:30 AM HDE  \n",
       "26   3:30 AM HDE  \n",
       "27   3:30 AM HDE  \n",
       "28   3:00 AM HDE  \n",
       "29   3:00 AM HDE  \n",
       "30   8:30 AM HDE  \n",
       "31   8:30 AM HDE  \n",
       "32   8:30 AM HDE  \n",
       "33  11:00 PM HDE  \n",
       "34  11:00 PM HDE  \n",
       "35  11:00 PM HDE  \n",
       "36  11:00 PM HDE  \n",
       "37  11:00 PM HDE  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f155df",
   "metadata": {},
   "source": [
    "3. Scrape the details of State-wise GDP of India from statisticstime.com. \n",
    "   \n",
    "   Url = http://statisticstimes.com/\n",
    "   \n",
    "   You have to find following details: \n",
    "        A)Rank \n",
    "        B) State \n",
    "        C) GSDP(18-19)- at current prices \n",
    "        D) GSDP(19-20)- at current prices \n",
    "        E) Share(18-19) \n",
    "        F) GDP($ billion) \n",
    "    Note: - From statisticstimes home page you have to reach to economy page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5e019d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pop-up removed\n",
      "Going to >> Economy >> India\n",
      "Removing advs\n",
      "Going to State-wise GDP of India\n",
      "Removing advs\n",
      "Columns: ['Rank', 'State', 'GSDP (Cr INR at Current prices)', 'Share', 'GDP ($billion)', 'GSDP (Cr INR at 2011-12 prices)']\n",
      "Recovered 33 records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP(19-20) - Current prices</th>\n",
       "      <th>GSDP(18-19) - Current prices</th>\n",
       "      <th>Share(18-19)</th>\n",
       "      <th>GDP ($billion)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>-</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>-</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,253,832</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>1,020,989</td>\n",
       "      <td>942,586</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>972,782</td>\n",
       "      <td>862,957</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>969,604</td>\n",
       "      <td>861,031</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>906,672</td>\n",
       "      <td>809,592</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>-</td>\n",
       "      <td>781,653</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>856,112</td>\n",
       "      <td>774,870</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>831,610</td>\n",
       "      <td>734,163</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>611,804</td>\n",
       "      <td>530,363</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>574,760</td>\n",
       "      <td>526,376</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>521,275</td>\n",
       "      <td>487,805</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>-</td>\n",
       "      <td>315,881</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>329,180</td>\n",
       "      <td>304,063</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>328,598</td>\n",
       "      <td>297,204</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>-</td>\n",
       "      <td>245,895</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>-</td>\n",
       "      <td>155,956</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>165,472</td>\n",
       "      <td>153,845</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>80,449</td>\n",
       "      <td>73,170</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>55,984</td>\n",
       "      <td>49,845</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>-</td>\n",
       "      <td>42,114</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>38,253</td>\n",
       "      <td>34,433</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>36,572</td>\n",
       "      <td>33,481</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>32,496</td>\n",
       "      <td>28,723</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>31,790</td>\n",
       "      <td>27,870</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>-</td>\n",
       "      <td>27,283</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>-</td>\n",
       "      <td>24,603</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>26,503</td>\n",
       "      <td>22,287</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      State GSDP(19-20) - Current prices  \\\n",
       "0     1                Maharashtra                            -   \n",
       "1     2                 Tamil Nadu                    1,845,853   \n",
       "2     3              Uttar Pradesh                    1,687,818   \n",
       "3     4                    Gujarat                            -   \n",
       "4     5                  Karnataka                    1,631,977   \n",
       "5     6                West Bengal                    1,253,832   \n",
       "6     7                  Rajasthan                    1,020,989   \n",
       "7     8             Andhra Pradesh                      972,782   \n",
       "8     9                  Telangana                      969,604   \n",
       "9    10             Madhya Pradesh                      906,672   \n",
       "10   11                     Kerala                            -   \n",
       "11   12                      Delhi                      856,112   \n",
       "12   13                    Haryana                      831,610   \n",
       "13   14                      Bihar                      611,804   \n",
       "14   15                     Punjab                      574,760   \n",
       "15   16                     Odisha                      521,275   \n",
       "16   17                      Assam                            -   \n",
       "17   18               Chhattisgarh                      329,180   \n",
       "18   19                  Jharkhand                      328,598   \n",
       "19   20                Uttarakhand                            -   \n",
       "20   21            Jammu & Kashmir                            -   \n",
       "21   22           Himachal Pradesh                      165,472   \n",
       "22   23                        Goa                       80,449   \n",
       "23   24                    Tripura                       55,984   \n",
       "24   25                 Chandigarh                            -   \n",
       "25   26                 Puducherry                       38,253   \n",
       "26   27                  Meghalaya                       36,572   \n",
       "27   28                     Sikkim                       32,496   \n",
       "28   29                    Manipur                       31,790   \n",
       "29   30                   Nagaland                            -   \n",
       "30   31          Arunachal Pradesh                            -   \n",
       "31   32                    Mizoram                       26,503   \n",
       "32   33  Andaman & Nicobar Islands                            -   \n",
       "\n",
       "   GSDP(18-19) - Current prices Share(18-19) GDP ($billion)  \n",
       "0                     2,632,792       13.94%        399.921  \n",
       "1                     1,630,208        8.63%        247.629  \n",
       "2                     1,584,764        8.39%        240.726  \n",
       "3                     1,502,899        7.96%        228.290  \n",
       "4                     1,493,127        7.91%        226.806  \n",
       "5                     1,089,898        5.77%        165.556  \n",
       "6                       942,586        4.99%        143.179  \n",
       "7                       862,957        4.57%        131.083  \n",
       "8                       861,031        4.56%        130.791  \n",
       "9                       809,592        4.29%        122.977  \n",
       "10                      781,653        4.14%        118.733  \n",
       "11                      774,870        4.10%        117.703  \n",
       "12                      734,163        3.89%        111.519  \n",
       "13                      530,363        2.81%         80.562  \n",
       "14                      526,376        2.79%         79.957  \n",
       "15                      487,805        2.58%         74.098  \n",
       "16                      315,881        1.67%         47.982  \n",
       "17                      304,063        1.61%         46.187  \n",
       "18                      297,204        1.57%         45.145  \n",
       "19                      245,895        1.30%         37.351  \n",
       "20                      155,956        0.83%         23.690  \n",
       "21                      153,845        0.81%         23.369  \n",
       "22                       73,170        0.39%         11.115  \n",
       "23                       49,845        0.26%          7.571  \n",
       "24                       42,114        0.22%          6.397  \n",
       "25                       34,433        0.18%          5.230  \n",
       "26                       33,481        0.18%          5.086  \n",
       "27                       28,723        0.15%          4.363  \n",
       "28                       27,870        0.15%          4.233  \n",
       "29                       27,283        0.14%          4.144  \n",
       "30                       24,603        0.13%          3.737  \n",
       "31                       22,287        0.12%          3.385  \n",
       "32                            -            -              -  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the Driver \n",
    "url = \"http://statisticstimes.com/\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url) \n",
    "\n",
    "wait_time = 3\n",
    "time.sleep(wait_time)\n",
    "remove_popup('//*[@aria-label=\"dismiss cookie message\"]')\n",
    "\n",
    "# Opening option Economy\n",
    "try:\n",
    "    time.sleep(wait_time)\n",
    "    # xpath_abs = '/html/body/div[2]/div[1]/div[2]/div[2]/button'\n",
    "    xpath_rel = '//button[contains(text(),\"Economy\")]'\n",
    "    economy_link = driver.find_element(By.XPATH, xpath_rel)\n",
    "    economy_link.click()\n",
    "\n",
    "    india_link = driver.find_element(By.XPATH,'//div[@class=\"dropdown-content\"]//a[3]')\n",
    "    india_link.click()\n",
    "    print(\"Going to >> Economy >> India\")\n",
    "    \n",
    "    # Removing advs\n",
    "    actions = ActionChains(driver)\n",
    "    x_coordinate = 300\n",
    "    y_coordinate = 200\n",
    "    # Moving cursor and click\n",
    "    actions.move_by_offset(x_coordinate, y_coordinate).click().perform()\n",
    "    print(\"Removing advs\")\n",
    "    \n",
    "except NoSuchElementException:\n",
    "    print(\"Option can't be found\")\n",
    "    \n",
    "try:\n",
    "    # Going to the India GDP page\n",
    "    time.sleep(wait_time)\n",
    "    xpath = '//a[contains(text(),\"GDP of Indian states\")]/ancestor::li'\n",
    "    link = driver.find_element(By.XPATH, xpath)\n",
    "    link.click()\n",
    "    print(\"Going to State-wise GDP of India\")\n",
    "except NoSuchElementException:\n",
    "    print(\"Link India GDP can't be found\")\n",
    "\n",
    "    \n",
    "try:\n",
    "    # Removing the advs\n",
    "    xpaths_adv = ['//*[@id=\"hideSlideBanner\"]']\n",
    "    el = driver.find_element(By.XPATH, xpaths_adv[0])\n",
    "    el.click()\n",
    "    el.get_attribute(\"innerHTML\")\n",
    "    print(\"Removing advs\")\n",
    "except NoSuchElementException:\n",
    "    print(\"No advs\")    \n",
    "\n",
    "    \n",
    "# Scraping the data\n",
    "xpath_table = '//table[@id=\"table_id\"]'\n",
    "df, columns = web_scraping_table(xpath_table, init_data=2, footer=True)\n",
    "\n",
    "# Cleaning the driver\n",
    "driver.quit()\n",
    "\n",
    "# Setting columns\n",
    "df.columns = ['Rank', 'State','GSDP(19-20) - Current prices', 'GSDP(18-19) - Current prices', 'Share(18-19)', 'GDP ($billion)', 'GSDP','GSDP']\n",
    "df.drop(['GSDP'], axis=1, inplace=True)\n",
    "\n",
    "# Saving the dataframe\n",
    "df.to_csv('datasets/q3_GSDP_India_states')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646bd8ff",
   "metadata": {},
   "source": [
    "4. Scrape the details of trending repositories on Github.com. \n",
    "\n",
    "    Url = https://github.com/\n",
    "\n",
    "    You have to find the following details: \n",
    "\n",
    "        A) Repository title \n",
    "        B) Repository description \n",
    "        C) Contributors count \n",
    "        D) Language used \n",
    "    Note: - From the home page you have to click on the trending option from Explore menu through code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "1516dd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open Source option...\n",
      "Going to Trending page...\n",
      "Repository Title retrieved and included: 25\n",
      "URL retrieved and included: 25\n",
      "The page could not be loaded completed within the specified time. 1/25, URL: https://github.com/a16z-infra/ai-town\n",
      "Retrieving Details of record: 2/25, url: https://github.com/opentffoundation/manifesto\n",
      "The page could not be loaded completed within the specified time. 3/25, URL: https://github.com/lllyasviel/Fooocus\n",
      "The page could not be loaded completed within the specified time. 4/25, URL: https://github.com/varunshenoy/opendream\n",
      "The page could not be loaded completed within the specified time. 5/25, URL: https://github.com/modelscope/facechain\n",
      "Retrieving Details of record: 6/25, url: https://github.com/Azure-Samples/azure-search-openai-demo\n",
      "The page could not be loaded completed within the specified time. 7/25, URL: https://github.com/chatchat-space/Langchain-Chatchat\n",
      "The page could not be loaded completed within the specified time. 8/25, URL: https://github.com/morph-labs/rift\n",
      "The page could not be loaded completed within the specified time. 9/25, URL: https://github.com/NVlabs/neuralangelo\n",
      "Retrieving Details of record: 10/25, url: https://github.com/ProfSynapse/Synapse_CoR\n",
      "Retrieving Details of record: 11/25, url: https://github.com/steven2358/awesome-generative-ai\n",
      "Retrieving Details of record: 12/25, url: https://github.com/ibaiw/2023Hvv\n",
      "Retrieving Details of record: 13/25, url: https://github.com/iam-veeramalla/aws-devops-zero-to-hero\n",
      "Retrieving Details of record: 14/25, url: https://github.com/AntonOsika/gpt-engineer\n",
      "Retrieving Details of record: 15/25, url: https://github.com/bigskysoftware/htmx\n",
      "Retrieving Details of record: 16/25, url: https://github.com/angular/angular\n",
      "The page could not be loaded completed within the specified time. 17/25, URL: https://github.com/XingangPan/DragGAN\n",
      "Retrieving Details of record: 18/25, url: https://github.com/google/googletest\n",
      "Retrieving Details of record: 19/25, url: https://github.com/Yidadaa/ChatGPT-Next-Web\n",
      "Retrieving Details of record: 20/25, url: https://github.com/awesome-selfhosted/awesome-selfhosted\n",
      "Retrieving Details of record: 21/25, url: https://github.com/vercel/next.js\n",
      "Retrieving Details of record: 22/25, url: https://github.com/1Panel-dev/1Panel\n",
      "Retrieving Details of record: 23/25, url: https://github.com/assafelovic/gpt-researcher\n",
      "Retrieving Details of record: 24/25, url: https://github.com/dotnet/maui\n",
      "Retrieving Details of record: 25/25, url: https://github.com/plbrault/youre-the-os\n",
      "Saving the data\n"
     ]
    }
   ],
   "source": [
    "# Open the Driver \n",
    "url = \"https://github.com/\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url) \n",
    "\n",
    "wait_time = 3\n",
    "time.sleep(wait_time)\n",
    "\n",
    "# Selecting Trending page\n",
    "try:\n",
    "    open_source_menu = driver.find_element(By.XPATH, '//ul[@class=\"d-lg-flex list-style-none\"]/li[3]')\n",
    "    open_source_menu.click()\n",
    "    print(\"Open Source option...\")\n",
    "\n",
    "    trending_link = driver.find_element(By.XPATH, '//*[contains(text(),\"Trending\")]')\n",
    "    trending_link.click()\n",
    "    print(\"Going to Trending page...\")\n",
    "    \n",
    "except NoSuchElementException as e:\n",
    "    print(f\"{e.msg}\")\n",
    "except Exception as e:\n",
    "    print(f\"{e.msg}\")\n",
    "    \n",
    "# Scraping the data: Repository title and URL for further scraping\n",
    "\n",
    "# Defining lambda functions to retrieved each element\n",
    "fn_text = lambda web: text_el(web)\n",
    "fn_href = lambda web: get_attribute_el(web,'href')\n",
    "\n",
    "# Configuring xpaths\n",
    "attributes = {'Repository Title': [By.XPATH, '//*[@class=\"Box-row\"]/h2', fn_text, False ],\n",
    "              'URL': [By.XPATH, '//a[@class=\"Link\"]', fn_href, False ]}\n",
    "\n",
    "attributes_det = {'Repository description': [By.XPATH, '//p[@class=\"f4 my-3\"]', fn_text, False],\n",
    "                  'Contributors count' : [By.XPATH, '//a[contains(text(),\"Contributors\")]/span', fn_text, False],\n",
    "                  'Language used': [By.XPATH, '//span[@class=\"color-fg-default text-bold mr-1\"]', fn_text, True]}\n",
    "\n",
    "# Retrieving Repository title and URL \n",
    "wait_time = 5\n",
    "time.sleep(wait_time)\n",
    "df_repo = load_items_by(driver, None, **attributes)\n",
    "\n",
    "# Scraping detail info\n",
    "wait_time = 5\n",
    "time.sleep(wait_time)\n",
    "df_det = scrape_data_by_urls(df_repo['URL'], attributes_det)\n",
    "\n",
    "# Cleaning the driver\n",
    "driver.quit()\n",
    "\n",
    "# Checking the dimensions of dataframes\n",
    "if len(df_det) == len(df_repo):\n",
    "    df_repo = pd.concat([df_repo, df_det], axis=1)\n",
    "\n",
    "# Saving the data frame\n",
    "df_repo.to_csv('datasets/q4_repositories.csv')\n",
    "print(\"Saving the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "046a86e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Repository description</th>\n",
       "      <th>Contributors count</th>\n",
       "      <th>Language used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a16z-infra / ai-town</td>\n",
       "      <td>https://github.com/a16z-infra/ai-town</td>\n",
       "      <td>A MIT-licensed, deployable starter kit for bui...</td>\n",
       "      <td>11</td>\n",
       "      <td>[TypeScript, JavaScript, CSS, Dockerfile]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>opentffoundation / manifesto</td>\n",
       "      <td>https://github.com/opentffoundation/manifesto</td>\n",
       "      <td>The OpenTF Manifesto expresses concern over Ha...</td>\n",
       "      <td>234</td>\n",
       "      <td>[HTML, CSS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lllyasviel / Fooocus</td>\n",
       "      <td>https://github.com/lllyasviel/Fooocus</td>\n",
       "      <td>Focus on prompting and generating</td>\n",
       "      <td>3</td>\n",
       "      <td>[Python, Jupyter Notebook]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>varunshenoy / opendream</td>\n",
       "      <td>https://github.com/varunshenoy/opendream</td>\n",
       "      <td>An extensible, easy-to-use, and portable diffu...</td>\n",
       "      <td>6</td>\n",
       "      <td>[JavaScript, Python, HTML, CSS, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>modelscope / facechain</td>\n",
       "      <td>https://github.com/modelscope/facechain</td>\n",
       "      <td>FaceChain is a deep-learning toolchain for gen...</td>\n",
       "      <td>7</td>\n",
       "      <td>[Python, Jupyter Notebook, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Azure-Samples / azure-search-openai-demo</td>\n",
       "      <td>https://github.com/Azure-Samples/azure-search-...</td>\n",
       "      <td>A sample app for the Retrieval-Augmented Gener...</td>\n",
       "      <td>35</td>\n",
       "      <td>[Python, TypeScript, Jupyter Notebook, Bicep, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chatchat-space / Langchain-Chatchat</td>\n",
       "      <td>https://github.com/chatchat-space/Langchain-Ch...</td>\n",
       "      <td>Langchain-Chatchat (formerly langchain-ChatGLM...</td>\n",
       "      <td>74</td>\n",
       "      <td>[Python, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>morph-labs / rift</td>\n",
       "      <td>https://github.com/morph-labs/rift</td>\n",
       "      <td>Rift: an AI-native language server for your pe...</td>\n",
       "      <td>13</td>\n",
       "      <td>[Python, TypeScript, Svelte, JavaScript, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NVlabs / neuralangelo</td>\n",
       "      <td>https://github.com/NVlabs/neuralangelo</td>\n",
       "      <td>Official implementation of \"Neuralangelo: High...</td>\n",
       "      <td>2</td>\n",
       "      <td>[Python, Shell, Jupyter Notebook]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ProfSynapse / Synapse_CoR</td>\n",
       "      <td>https://github.com/ProfSynapse/Synapse_CoR</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>steven2358 / awesome-generative-ai</td>\n",
       "      <td>https://github.com/steven2358/awesome-generati...</td>\n",
       "      <td>A curated list of modern Generative Artificial...</td>\n",
       "      <td>27</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ibaiw / 2023Hvv</td>\n",
       "      <td>https://github.com/ibaiw/2023Hvv</td>\n",
       "      <td>2023 HVV情报速递~</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>iam-veeramalla / aws-devops-zero-to-hero</td>\n",
       "      <td>https://github.com/iam-veeramalla/aws-devops-z...</td>\n",
       "      <td>AWS zero to hero repo for devops engineers to ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[Python, HCL, Shell, Dockerfile]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AntonOsika / gpt-engineer</td>\n",
       "      <td>https://github.com/AntonOsika/gpt-engineer</td>\n",
       "      <td>Specify what you want it to build, the AI asks...</td>\n",
       "      <td>57</td>\n",
       "      <td>[Python, Makefile, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bigskysoftware / htmx</td>\n",
       "      <td>https://github.com/bigskysoftware/htmx</td>\n",
       "      <td>&lt;/&gt; htmx - high power tools for HTML</td>\n",
       "      <td>215</td>\n",
       "      <td>[JavaScript, HTML, Go, CSS, Ruby, TypeScript]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>angular / angular</td>\n",
       "      <td>https://github.com/angular/angular</td>\n",
       "      <td>The modern web developer’s platform</td>\n",
       "      <td>1,767</td>\n",
       "      <td>[TypeScript, JavaScript, Starlark, HTML, CSS, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>XingangPan / DragGAN</td>\n",
       "      <td>https://github.com/XingangPan/DragGAN</td>\n",
       "      <td>Official Code for DragGAN (SIGGRAPH 2023)</td>\n",
       "      <td>16</td>\n",
       "      <td>[Python, Cuda, C++]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>google / googletest</td>\n",
       "      <td>https://github.com/google/googletest</td>\n",
       "      <td>GoogleTest - Google Testing and Mocking Framework</td>\n",
       "      <td>406</td>\n",
       "      <td>[C++, Python, CMake, C, Starlark, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Yidadaa / ChatGPT-Next-Web</td>\n",
       "      <td>https://github.com/Yidadaa/ChatGPT-Next-Web</td>\n",
       "      <td>A well-designed cross-platform ChatGPT UI (Web...</td>\n",
       "      <td>118</td>\n",
       "      <td>[TypeScript, SCSS, JavaScript, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>awesome-selfhosted / awesome-selfhosted</td>\n",
       "      <td>https://github.com/awesome-selfhosted/awesome-...</td>\n",
       "      <td>A list of Free Software network services and w...</td>\n",
       "      <td>1,254</td>\n",
       "      <td>[Makefile]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>vercel / next.js</td>\n",
       "      <td>https://github.com/vercel/next.js</td>\n",
       "      <td>The React Framework</td>\n",
       "      <td>2,824</td>\n",
       "      <td>[JavaScript, TypeScript, Rust, MDX, CSS, SCSS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1Panel-dev / 1Panel</td>\n",
       "      <td>https://github.com/1Panel-dev/1Panel</td>\n",
       "      <td>🔥 🔥 🔥 现代化、开源的 Linux 服务器运维管理面板。</td>\n",
       "      <td>28</td>\n",
       "      <td>[Go, Vue, SCSS, JavaScript, HTML, CSS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>assafelovic / gpt-researcher</td>\n",
       "      <td>https://github.com/assafelovic/gpt-researcher</td>\n",
       "      <td>GPT based autonomous agent that does online co...</td>\n",
       "      <td>12</td>\n",
       "      <td>[Python, HTML, JavaScript, CSS, Dockerfile]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>dotnet / maui</td>\n",
       "      <td>https://github.com/dotnet/maui</td>\n",
       "      <td>.NET MAUI is the .NET Multi-platform App UI, a...</td>\n",
       "      <td>509</td>\n",
       "      <td>[C#, PowerShell, Java, Shell, HTML, CSS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>plbrault / youre-the-os</td>\n",
       "      <td>https://github.com/plbrault/youre-the-os</td>\n",
       "      <td>A game where you are a computer's OS and you h...</td>\n",
       "      <td>2</td>\n",
       "      <td>[Python]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Repository Title  \\\n",
       "0                       a16z-infra / ai-town   \n",
       "1               opentffoundation / manifesto   \n",
       "2                       lllyasviel / Fooocus   \n",
       "3                    varunshenoy / opendream   \n",
       "4                     modelscope / facechain   \n",
       "5   Azure-Samples / azure-search-openai-demo   \n",
       "6        chatchat-space / Langchain-Chatchat   \n",
       "7                          morph-labs / rift   \n",
       "8                      NVlabs / neuralangelo   \n",
       "9                  ProfSynapse / Synapse_CoR   \n",
       "10        steven2358 / awesome-generative-ai   \n",
       "11                           ibaiw / 2023Hvv   \n",
       "12  iam-veeramalla / aws-devops-zero-to-hero   \n",
       "13                 AntonOsika / gpt-engineer   \n",
       "14                     bigskysoftware / htmx   \n",
       "15                         angular / angular   \n",
       "16                      XingangPan / DragGAN   \n",
       "17                       google / googletest   \n",
       "18                Yidadaa / ChatGPT-Next-Web   \n",
       "19   awesome-selfhosted / awesome-selfhosted   \n",
       "20                          vercel / next.js   \n",
       "21                       1Panel-dev / 1Panel   \n",
       "22              assafelovic / gpt-researcher   \n",
       "23                             dotnet / maui   \n",
       "24                   plbrault / youre-the-os   \n",
       "\n",
       "                                                  URL  \\\n",
       "0               https://github.com/a16z-infra/ai-town   \n",
       "1       https://github.com/opentffoundation/manifesto   \n",
       "2               https://github.com/lllyasviel/Fooocus   \n",
       "3            https://github.com/varunshenoy/opendream   \n",
       "4             https://github.com/modelscope/facechain   \n",
       "5   https://github.com/Azure-Samples/azure-search-...   \n",
       "6   https://github.com/chatchat-space/Langchain-Ch...   \n",
       "7                  https://github.com/morph-labs/rift   \n",
       "8              https://github.com/NVlabs/neuralangelo   \n",
       "9          https://github.com/ProfSynapse/Synapse_CoR   \n",
       "10  https://github.com/steven2358/awesome-generati...   \n",
       "11                   https://github.com/ibaiw/2023Hvv   \n",
       "12  https://github.com/iam-veeramalla/aws-devops-z...   \n",
       "13         https://github.com/AntonOsika/gpt-engineer   \n",
       "14             https://github.com/bigskysoftware/htmx   \n",
       "15                 https://github.com/angular/angular   \n",
       "16              https://github.com/XingangPan/DragGAN   \n",
       "17               https://github.com/google/googletest   \n",
       "18        https://github.com/Yidadaa/ChatGPT-Next-Web   \n",
       "19  https://github.com/awesome-selfhosted/awesome-...   \n",
       "20                  https://github.com/vercel/next.js   \n",
       "21               https://github.com/1Panel-dev/1Panel   \n",
       "22      https://github.com/assafelovic/gpt-researcher   \n",
       "23                     https://github.com/dotnet/maui   \n",
       "24           https://github.com/plbrault/youre-the-os   \n",
       "\n",
       "                               Repository description Contributors count  \\\n",
       "0   A MIT-licensed, deployable starter kit for bui...                 11   \n",
       "1   The OpenTF Manifesto expresses concern over Ha...                234   \n",
       "2                   Focus on prompting and generating                  3   \n",
       "3   An extensible, easy-to-use, and portable diffu...                  6   \n",
       "4   FaceChain is a deep-learning toolchain for gen...                  7   \n",
       "5   A sample app for the Retrieval-Augmented Gener...                 35   \n",
       "6   Langchain-Chatchat (formerly langchain-ChatGLM...                 74   \n",
       "7   Rift: an AI-native language server for your pe...                 13   \n",
       "8   Official implementation of \"Neuralangelo: High...                  2   \n",
       "9                                                   -                  -   \n",
       "10  A curated list of modern Generative Artificial...                 27   \n",
       "11                                      2023 HVV情报速递~                  -   \n",
       "12  AWS zero to hero repo for devops engineers to ...                  2   \n",
       "13  Specify what you want it to build, the AI asks...                 57   \n",
       "14               </> htmx - high power tools for HTML                215   \n",
       "15                The modern web developer’s platform              1,767   \n",
       "16          Official Code for DragGAN (SIGGRAPH 2023)                 16   \n",
       "17  GoogleTest - Google Testing and Mocking Framework                406   \n",
       "18  A well-designed cross-platform ChatGPT UI (Web...                118   \n",
       "19  A list of Free Software network services and w...              1,254   \n",
       "20                                The React Framework              2,824   \n",
       "21                     🔥 🔥 🔥 现代化、开源的 Linux 服务器运维管理面板。                 28   \n",
       "22  GPT based autonomous agent that does online co...                 12   \n",
       "23  .NET MAUI is the .NET Multi-platform App UI, a...                509   \n",
       "24  A game where you are a computer's OS and you h...                  2   \n",
       "\n",
       "                                        Language used  \n",
       "0           [TypeScript, JavaScript, CSS, Dockerfile]  \n",
       "1                                         [HTML, CSS]  \n",
       "2                          [Python, Jupyter Notebook]  \n",
       "3              [JavaScript, Python, HTML, CSS, Shell]  \n",
       "4                   [Python, Jupyter Notebook, Shell]  \n",
       "5   [Python, TypeScript, Jupyter Notebook, Bicep, ...  \n",
       "6                                     [Python, Shell]  \n",
       "7     [Python, TypeScript, Svelte, JavaScript, Other]  \n",
       "8                   [Python, Shell, Jupyter Notebook]  \n",
       "9                                                   -  \n",
       "10                                                  -  \n",
       "11                                                  -  \n",
       "12                   [Python, HCL, Shell, Dockerfile]  \n",
       "13                          [Python, Makefile, Other]  \n",
       "14      [JavaScript, HTML, Go, CSS, Ruby, TypeScript]  \n",
       "15  [TypeScript, JavaScript, Starlark, HTML, CSS, ...  \n",
       "16                                [Python, Cuda, C++]  \n",
       "17           [C++, Python, CMake, C, Starlark, Shell]  \n",
       "18              [TypeScript, SCSS, JavaScript, Other]  \n",
       "19                                         [Makefile]  \n",
       "20     [JavaScript, TypeScript, Rust, MDX, CSS, SCSS]  \n",
       "21             [Go, Vue, SCSS, JavaScript, HTML, CSS]  \n",
       "22        [Python, HTML, JavaScript, CSS, Dockerfile]  \n",
       "23           [C#, PowerShell, Java, Shell, HTML, CSS]  \n",
       "24                                           [Python]  "
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d754ef",
   "metadata": {},
   "source": [
    "5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/\n",
    "    You have to find the following details: \n",
    "        A) Song name \n",
    "        B) Artist name \n",
    "        C) Last week rank \n",
    "        D) Peak rank \n",
    "        E) Weeks on board \n",
    "    Note: - From the home page you have to click on the charts option then hot 100-page link through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "23f887fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pop-up removed\n",
      "Selecting CHARTS...\n",
      "Selecting Billboard Hot 100...\n"
     ]
    }
   ],
   "source": [
    "# Open the Driver \n",
    "url = \"https:/www.billboard.com/\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url) \n",
    "\n",
    "# Wait for the results to load\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "\n",
    "remove_popup('//a[@class=\"btn-close\"]')\n",
    "\n",
    "# Selecting Charts\n",
    "xpath = '//*[@class=\"o-nav  \"]/ul/li[1]/a'\n",
    "charts_link = driver.find_element(By.XPATH, xpath).get_attribute('href')\n",
    "driver.get(charts_link)\n",
    "print(\"Selecting CHARTS...\")\n",
    "\n",
    "wait_time = 5\n",
    "time.sleep(wait_time)\n",
    "# Selecting Billboard Hot 100\n",
    "time.sleep(wait_time)\n",
    "hot100_link = driver.find_element(By.XPATH, '//span[@class=\"c-label  \"]')\n",
    "hot100_link.click()\n",
    "print(\"Selecting Billboard Hot 100...\")\n",
    "\n",
    "# Retrieve elements using Selenium\n",
    "time.sleep(wait_time)\n",
    "xpath_grp = '//*[@class=\"o-chart-results-list-row-container\"]'\n",
    "elements = driver.find_elements(By.XPATH, xpath_grp)\n",
    "\n",
    "# Convert the elements to HTML strings\n",
    "html_strings = [element.get_attribute(\"outerHTML\") for element in elements]\n",
    "print(f\"Records retrieved: {len(html_strings)}\")\n",
    "      \n",
    "# Close the Selenium driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "4fc8f729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>song_name</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>last_week_rank</th>\n",
       "      <th>peak_rank</th>\n",
       "      <th>weeks_on_board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Last Night</td>\n",
       "      <td>Last Night\\t\\t\\n\\t\\n\\n\\n\\t\\n\\tMorgan Wallen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Fast Car</td>\n",
       "      <td>Fast Car\\t\\t\\n\\t\\n\\n\\n\\t\\n\\tLuke Combs</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Cruel Summer</td>\n",
       "      <td>Cruel Summer\\t\\t\\n\\t\\n\\n\\n\\t\\n\\tTaylor Swift</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Calm Down</td>\n",
       "      <td>Calm Down\\t\\t\\n\\t\\n\\n\\n\\t\\n\\tRema &amp; Selena Gomez</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fukumean</td>\n",
       "      <td>Fukumean\\t\\t\\n\\t\\n\\n\\n\\t\\n\\tGunna</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Lagunas</td>\n",
       "      <td>Lagunas\\t\\t\\n\\t\\n\\n\\n\\t\\n\\tPeso Pluma &amp; Jasiel...</td>\n",
       "      <td>-</td>\n",
       "      <td>77</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Overdrive</td>\n",
       "      <td>Overdrive\\t\\t\\n\\t\\n\\n\\n\\t\\n\\tPost Malone</td>\n",
       "      <td>68</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Bzrp Music Sessions, Vol. 55</td>\n",
       "      <td>Bzrp Music Sessions, Vol. 55\\t\\t\\n\\t\\n\\n\\n\\t\\n...</td>\n",
       "      <td>99</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Dawns</td>\n",
       "      <td>Dawns\\t\\t\\n\\t\\n\\n\\n\\t\\n\\tZach Bryan Featuring ...</td>\n",
       "      <td>-</td>\n",
       "      <td>42</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Rubicon</td>\n",
       "      <td>Rubicon\\t\\t\\n\\t\\n\\n\\n\\t\\n\\tPeso Pluma</td>\n",
       "      <td>-</td>\n",
       "      <td>63</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank                     song_name  \\\n",
       "0     1                    Last Night   \n",
       "1     2                      Fast Car   \n",
       "2     3                  Cruel Summer   \n",
       "3     4                     Calm Down   \n",
       "4     5                      Fukumean   \n",
       "..  ...                           ...   \n",
       "95   96                       Lagunas   \n",
       "96   97                     Overdrive   \n",
       "97   98  Bzrp Music Sessions, Vol. 55   \n",
       "98   99                         Dawns   \n",
       "99  100                       Rubicon   \n",
       "\n",
       "                                          artist_name last_week_rank  \\\n",
       "0         Last Night\\t\\t\\n\\t\\n\\n\\n\\t\\n\\tMorgan Wallen              1   \n",
       "1              Fast Car\\t\\t\\n\\t\\n\\n\\n\\t\\n\\tLuke Combs              2   \n",
       "2        Cruel Summer\\t\\t\\n\\t\\n\\n\\n\\t\\n\\tTaylor Swift              4   \n",
       "3    Calm Down\\t\\t\\n\\t\\n\\n\\n\\t\\n\\tRema & Selena Gomez              6   \n",
       "4                   Fukumean\\t\\t\\n\\t\\n\\n\\n\\t\\n\\tGunna              7   \n",
       "..                                                ...            ...   \n",
       "95  Lagunas\\t\\t\\n\\t\\n\\n\\n\\t\\n\\tPeso Pluma & Jasiel...              -   \n",
       "96           Overdrive\\t\\t\\n\\t\\n\\n\\n\\t\\n\\tPost Malone             68   \n",
       "97  Bzrp Music Sessions, Vol. 55\\t\\t\\n\\t\\n\\n\\n\\t\\n...             99   \n",
       "98  Dawns\\t\\t\\n\\t\\n\\n\\n\\t\\n\\tZach Bryan Featuring ...              -   \n",
       "99              Rubicon\\t\\t\\n\\t\\n\\n\\n\\t\\n\\tPeso Pluma              -   \n",
       "\n",
       "   peak_rank weeks_on_board  \n",
       "0          1             28  \n",
       "1          2             20  \n",
       "2          3             14  \n",
       "3          3             49  \n",
       "4          4              8  \n",
       "..       ...            ...  \n",
       "95        77              6  \n",
       "96        47              3  \n",
       "97        31             10  \n",
       "98        42             15  \n",
       "99        63              6  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scraping data using BeautifulSoup for convenience\n",
    "rank = []\n",
    "song_name = []\n",
    "artist_name = []\n",
    "last_week_rank = []\n",
    "peak_rank = []\n",
    "weeks_on_board = []\n",
    "\n",
    "for html_string in html_strings:\n",
    "    soup = BeautifulSoup(html_string, 'html.parser')\n",
    "    \n",
    "    rank.append(soup.find('span').text.strip())\n",
    "    song_name.append(soup.find('h3').text.strip())\n",
    "\n",
    "    lis = soup.find_all('li', class_=\"o-chart-results-list__item\" )\n",
    "    artist_name.append(lis[3].text.strip())\n",
    "    last_week_rank.append(lis[6].text.strip())\n",
    "    peak_rank.append(lis[7].text.strip())\n",
    "    weeks_on_board.append(lis[8].text.strip())\n",
    "    \n",
    "    df = pd.DataFrame({'rank': rank ,\n",
    "                   'song_name' :song_name,\n",
    "                   'artist_name': artist_name,\n",
    "                   'last_week_rank': last_week_rank,\n",
    "                   'peak_rank': peak_rank,\n",
    "                   'weeks_on_board': weeks_on_board})\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "2eff050a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_name</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>last_week_rank</th>\n",
       "      <th>peak_rank</th>\n",
       "      <th>weeks_on_board</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Last Night</td>\n",
       "      <td>Morgan Wallen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fast Car</td>\n",
       "      <td>Luke Combs</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cruel Summer</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Calm Down</td>\n",
       "      <td>Rema &amp; Selena Gomez</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fukumean</td>\n",
       "      <td>Gunna</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Lagunas</td>\n",
       "      <td>Peso Pluma &amp; Jasiel Nunez</td>\n",
       "      <td>-</td>\n",
       "      <td>77</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Overdrive</td>\n",
       "      <td>Post Malone</td>\n",
       "      <td>68</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Bzrp Music Sessions, Vol. 55</td>\n",
       "      <td>Bizarrap &amp; Peso Pluma</td>\n",
       "      <td>99</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Dawns</td>\n",
       "      <td>Zach Bryan Featuring Maggie Rogers</td>\n",
       "      <td>-</td>\n",
       "      <td>42</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Rubicon</td>\n",
       "      <td>Peso Pluma</td>\n",
       "      <td>-</td>\n",
       "      <td>63</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         song_name                         artist_name  \\\n",
       "rank                                                                     \n",
       "1                       Last Night                       Morgan Wallen   \n",
       "2                         Fast Car                          Luke Combs   \n",
       "3                     Cruel Summer                        Taylor Swift   \n",
       "4                        Calm Down                 Rema & Selena Gomez   \n",
       "5                         Fukumean                               Gunna   \n",
       "...                            ...                                 ...   \n",
       "96                         Lagunas           Peso Pluma & Jasiel Nunez   \n",
       "97                       Overdrive                         Post Malone   \n",
       "98    Bzrp Music Sessions, Vol. 55               Bizarrap & Peso Pluma   \n",
       "99                           Dawns  Zach Bryan Featuring Maggie Rogers   \n",
       "100                        Rubicon                          Peso Pluma   \n",
       "\n",
       "     last_week_rank peak_rank weeks_on_board  \n",
       "rank                                          \n",
       "1                 1         1             28  \n",
       "2                 2         2             20  \n",
       "3                 4         3             14  \n",
       "4                 6         3             49  \n",
       "5                 7         4              8  \n",
       "...             ...       ...            ...  \n",
       "96                -        77              6  \n",
       "97               68        47              3  \n",
       "98               99        31             10  \n",
       "99                -        42             15  \n",
       "100               -        63              6  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning the data frame\n",
    "df['artist_name'] = df['artist_name'].str.extract(r'[ \\t]*([^\\n\\t]+)$')\n",
    "df.set_index('rank', drop=True, inplace=True)\n",
    "\n",
    "# Saving the data frame\n",
    "df.to_csv('datasets/q5_100top_songs.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d41343",
   "metadata": {},
   "source": [
    "6. Scrape the details of Highest selling novels. \n",
    "    \n",
    "    url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\n",
    "    \n",
    "    You have to find the following details:\n",
    "        A) Book name \n",
    "        B) Author name \n",
    "        C) Volumes sold \n",
    "        D) Publisher \n",
    "        E) Genre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "a68b6925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['Rank', 'Title', 'Author', 'Volume Sales', 'Publisher', 'Genre']\n",
      "Recovered 101 records.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book name</th>\n",
       "      <th>Author name</th>\n",
       "      <th>Volumnes sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Book name       Author name  \\\n",
       "Rank                                                                        \n",
       "1                                     Da Vinci Code,The        Brown, Dan   \n",
       "2                  Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "3              Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "4             Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "5                                  Fifty Shades of Grey      James, E. L.   \n",
       "...                                                 ...               ...   \n",
       "96                                            Ghost,The    Harris, Robert   \n",
       "97                       Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "98                Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "99    Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "100   Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "     Volumnes sold        Publisher                        Genre  \n",
       "Rank                                                              \n",
       "1        5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "2        4,475,152       Bloomsbury           Children's Fiction  \n",
       "3        4,200,654       Bloomsbury           Children's Fiction  \n",
       "4        4,179,479       Bloomsbury           Children's Fiction  \n",
       "5        3,758,936     Random House              Romance & Sagas  \n",
       "...            ...              ...                          ...  \n",
       "96         807,311     Random House   General & Literary Fiction  \n",
       "97         794,201          Penguin        Food & Drink: General  \n",
       "98         792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "99         791,507            Orion           Biography: General  \n",
       "100        791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Open the driver\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url) \n",
    "\n",
    "# Wait for the results to load\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Scraping the table \n",
    "xpath_table = '//table[@class=\"in-article sortable\"]'\n",
    "df, columns = web_scraping_table(xpath_table, footer=False)\n",
    "\n",
    "# cleaning driver\n",
    "driver.quit()\n",
    "\n",
    "# Setting up the dataframe\n",
    "df = df.iloc[1:] # deleting the first row that has not valid data\n",
    "df.columns = ['Rank', 'Book name', 'Author name', 'Volumnes sold', 'Publisher', 'Genre']\n",
    "df.set_index(['Rank'], inplace=True)\n",
    "\n",
    "# Saving the data frame\n",
    "df.to_csv('datasets/q6_Highest_selling_novels.csv')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4cafe9",
   "metadata": {},
   "source": [
    "7. Scrape the details most watched tv series of all time from imdb.com. \n",
    "\n",
    "    Url = https://www.imdb.com/list/ls095964455/ \n",
    "\n",
    "    You have to find the following details: \n",
    "\n",
    "        A) Name \n",
    "        B) Year span \n",
    "        C) Genre \n",
    "        D) Run time \n",
    "        E) Ratings \n",
    "        F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "ebde34b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name retrieved and included: 100\n",
      "Year span retrieved and included: 100\n",
      "Genre retrieved and included: 100\n",
      "Run time retrieved and included: 100\n",
      "Ratings retrieved and included: 100\n",
      "Votes retrieved and included: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Run time</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Juego de tronos</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9,2</td>\n",
       "      <td>2.193.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016–2024)</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8,7</td>\n",
       "      <td>1.267.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010–2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8,1</td>\n",
       "      <td>1.041.373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Por trece razones</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7,5</td>\n",
       "      <td>306.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Los 100</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7,6</td>\n",
       "      <td>265.151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Reign</td>\n",
       "      <td>(2013–2017)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7,5</td>\n",
       "      <td>52.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Una serie de catastróficas desdichas</td>\n",
       "      <td>(2017–2019)</td>\n",
       "      <td>Adventure, Comedy, Drama</td>\n",
       "      <td>50 min</td>\n",
       "      <td>7,8</td>\n",
       "      <td>64.483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Mentes criminales</td>\n",
       "      <td>(2005– )</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8,1</td>\n",
       "      <td>210.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Scream</td>\n",
       "      <td>(2015–2019)</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7</td>\n",
       "      <td>43.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>La maldición de Hill House</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>Drama, Horror, Mystery</td>\n",
       "      <td>572 min</td>\n",
       "      <td>8,6</td>\n",
       "      <td>263.814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Name    Year span  \\\n",
       "0                        Juego de tronos  (2011–2019)   \n",
       "1                        Stranger Things  (2016–2024)   \n",
       "2                       The Walking Dead  (2010–2022)   \n",
       "3                      Por trece razones  (2017–2020)   \n",
       "4                                Los 100  (2014–2020)   \n",
       "..                                   ...          ...   \n",
       "95                                 Reign  (2013–2017)   \n",
       "96  Una serie de catastróficas desdichas  (2017–2019)   \n",
       "97                     Mentes criminales     (2005– )   \n",
       "98                                Scream  (2015–2019)   \n",
       "99            La maldición de Hill House       (2018)   \n",
       "\n",
       "                       Genre Run time Ratings      Votes  \n",
       "0   Action, Adventure, Drama   57 min     9,2  2.193.972  \n",
       "1     Drama, Fantasy, Horror   51 min     8,7  1.267.622  \n",
       "2    Drama, Horror, Thriller   44 min     8,1  1.041.373  \n",
       "3   Drama, Mystery, Thriller   60 min     7,5    306.079  \n",
       "4     Drama, Mystery, Sci-Fi   43 min     7,6    265.151  \n",
       "..                       ...      ...     ...        ...  \n",
       "95                     Drama   42 min     7,5     52.464  \n",
       "96  Adventure, Comedy, Drama   50 min     7,8     64.483  \n",
       "97     Crime, Drama, Mystery   42 min     8,1    210.175  \n",
       "98      Comedy, Crime, Drama   45 min       7     43.698  \n",
       "99    Drama, Horror, Mystery  572 min     8,6    263.814  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Open the driver\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url) \n",
    "\n",
    "# Wait for the results to load\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Scraping movies\n",
    "fn_text = lambda web: text_el(web)\n",
    "attributes = {'Name': [By.XPATH, '//*[@class=\"lister-item-header\"]/a', fn_text, False],\n",
    "             'Year span': [By.XPATH, '//*[@class=\"lister-item-year text-muted unbold\"]', fn_text, False],\n",
    "             'Genre': [By.XPATH, '//*[@class=\"genre\"]', fn_text, False],\n",
    "             'Run time':[By.XPATH, '//*[@class=\"runtime\"]', fn_text, False],\n",
    "             'Ratings': [By.XPATH, '//*[@class=\"ipl-rating-star small\"]/span[2]', fn_text, False],\n",
    "             'Votes': [By.XPATH, '//*[@name=\"nv\"]', fn_text, False]}\n",
    "\n",
    "df = load_items_by(driver, None, **attributes)\n",
    "\n",
    "# Cleaning the driver\n",
    "driver.quit()\n",
    "\n",
    "# Saving data frame\n",
    "df.to_csv('datasets/q7_movies_.csv')\n",
    "\n",
    "# Display data frame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98cf2d",
   "metadata": {},
   "source": [
    "8. Details of Datasets from UCI machine learning repositories. \n",
    "\n",
    "    Url = https://archive.ics.uci.edu/\n",
    "\n",
    "    You have to find the following details: \n",
    "\n",
    "        A) Dataset name \n",
    "        B) Data type \n",
    "        C) Task \n",
    "        D) Attribute type \n",
    "        E) No of instances \n",
    "        F) No of attribute \n",
    "        G) Year \n",
    "        Note: - from the home page you have to go to the Show All Dataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "94c9a23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pop-up removed\n",
      "Going to DATASETS...\n",
      "Rows per page:25\n",
      "643 datasets to retrieve...\n"
     ]
    }
   ],
   "source": [
    "# Open the driver\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url) \n",
    "\n",
    "# Wait for the results to load\n",
    "wait_time = 3\n",
    "time.sleep(wait_time)\n",
    "\n",
    "xpath_btn = '//*[@class=\"btn-primary btn-sm btn m-1\"]'\n",
    "remove_popup(xpath_btn)\n",
    "\n",
    "# Selecting VIEW DATASETS\n",
    "try:\n",
    "    xpath_viewds = '//*[contains(text(),\"View Datasets\")]'\n",
    "    btn_view_ds = driver.find_element(By.XPATH, xpath_viewds)\n",
    "    btn_view_ds.get_attribute('innerHTML')\n",
    "    btn_view_ds.click()\n",
    "    print(\"Going to DATASETS...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Option VIEW DATASETS not found. {e.msg}\")\n",
    "    print(\"Going directly to DATASETS\")\n",
    "    driver.get( url + \"datasets\")\n",
    "    \n",
    "wait_time = 5\n",
    "time.sleep(wait_time)\n",
    "\n",
    "# Selecting 25 records by page \n",
    "select_element = driver.find_element(By.CLASS_NAME, \"select-primary\")\n",
    "select = Select(select_element)\n",
    "select.select_by_value(\"25\")\n",
    "selected_option = select.first_selected_option\n",
    "print(f\"Rows per page:{selected_option.text}\")\n",
    "\n",
    "# Expand all datasets\n",
    "xpath_swap = '//*[@class=\"swap\"]'\n",
    "expand_btn = driver.find_element(By.XPATH, xpath_swap)\n",
    "expand_btn.click()\n",
    "\n",
    "# Retriving the total of records\n",
    "try:\n",
    "    xpath_total = '//*[contains(text(),\"Rows per page\")]/following::p/span[3]'\n",
    "    total_ds_wel = driver.find_element(By.XPATH, xpath_total)\n",
    "    total_ds = int(total_ds_wel.text)\n",
    "    print(f\"{total_ds} datasets to retrieve...\")\n",
    "except NoSuchElementException:\n",
    "    total_ds = 0\n",
    "    print(\"No datasets to retrieve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa40acd",
   "metadata": {},
   "source": [
    "##### Scraping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "3227ee39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "25/643 datasets retrieved\n",
      " Going to \n",
      "page: 2\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "50/643 datasets retrieved\n",
      " Going to \n",
      "page: 3\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "75/643 datasets retrieved\n",
      " Going to \n",
      "page: 4\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "100/643 datasets retrieved\n",
      " Going to \n",
      "page: 5\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "125/643 datasets retrieved\n",
      " Going to \n",
      "page: 6\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "150/643 datasets retrieved\n",
      " Going to \n",
      "page: 7\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "175/643 datasets retrieved\n",
      " Going to \n",
      "page: 8\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "200/643 datasets retrieved\n",
      " Going to \n",
      "page: 9\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "225/643 datasets retrieved\n",
      " Going to \n",
      "page: 10\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "250/643 datasets retrieved\n",
      " Going to \n",
      "page: 11\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "275/643 datasets retrieved\n",
      " Going to \n",
      "page: 12\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "300/643 datasets retrieved\n",
      " Going to \n",
      "page: 13\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "325/643 datasets retrieved\n",
      " Going to \n",
      "page: 14\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "350/643 datasets retrieved\n",
      " Going to \n",
      "page: 15\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "375/643 datasets retrieved\n",
      " Going to \n",
      "page: 16\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "400/643 datasets retrieved\n",
      " Going to \n",
      "page: 17\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "425/643 datasets retrieved\n",
      " Going to \n",
      "page: 18\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "450/643 datasets retrieved\n",
      " Going to \n",
      "page: 19\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "475/643 datasets retrieved\n",
      " Going to \n",
      "page: 20\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "500/643 datasets retrieved\n",
      " Going to \n",
      "page: 21\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "525/643 datasets retrieved\n",
      " Going to \n",
      "page: 22\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "550/643 datasets retrieved\n",
      " Going to \n",
      "page: 23\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "575/643 datasets retrieved\n",
      " Going to \n",
      "page: 24\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "600/643 datasets retrieved\n",
      " Going to \n",
      "page: 25\n",
      "Dataset name retrieved and included: 25\n",
      "Data type retrieved and included: 25\n",
      "Task retrieved and included: 25\n",
      "Attribute type retrieved and included: 25\n",
      "No of instances retrieved and included: 25\n",
      "No of attribute retrieved and included: 25\n",
      "Date Donated retrieved and included: 25\n",
      "625/643 datasets retrieved\n",
      " Going to \n",
      "page: 26\n",
      "Dataset name retrieved and included: 18\n",
      "Data type retrieved and included: 18\n",
      "Task retrieved and included: 18\n",
      "Attribute type retrieved and included: 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of instances retrieved and included: 18\n",
      "No of attribute retrieved and included: 18\n",
      "Date Donated retrieved and included: 18\n",
      "643/643 datasets retrieved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset name</th>\n",
       "      <th>Data type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Attribute type</th>\n",
       "      <th>No of instances</th>\n",
       "      <th>No of attribute</th>\n",
       "      <th>Date Donated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>150 Instances</td>\n",
       "      <td>4 Attributes</td>\n",
       "      <td>6/30/1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heart Disease</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>303 Instances</td>\n",
       "      <td>13 Attributes</td>\n",
       "      <td>6/30/1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>48.84K Instances</td>\n",
       "      <td>14 Attributes</td>\n",
       "      <td>4/30/1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dry Bean Dataset</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>13.61K Instances</td>\n",
       "      <td>16 Attributes</td>\n",
       "      <td>9/13/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diabetes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td></td>\n",
       "      <td>20 Attributes</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Influenza outbreak event prediction via Twitter</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real, Integer</td>\n",
       "      <td>75.84K Instances</td>\n",
       "      <td>523 Attributes</td>\n",
       "      <td>8/14/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Average Localization Error (ALE) in sensor nod...</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>107 Instances</td>\n",
       "      <td>6 Attributes</td>\n",
       "      <td>8/13/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Turkish Music Emotion</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real, Integer</td>\n",
       "      <td>400 Instances</td>\n",
       "      <td>50 Attributes</td>\n",
       "      <td>8/14/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Gender Gap in Spanish WP</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real, Integer</td>\n",
       "      <td>4.75K Instances</td>\n",
       "      <td>21 Attributes</td>\n",
       "      <td>8/13/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Raisin</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real, Integer</td>\n",
       "      <td>900 Instances</td>\n",
       "      <td>8 Attributes</td>\n",
       "      <td>8/13/2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>643 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Dataset name     Data type  \\\n",
       "0                                                Iris  Multivariate   \n",
       "1                                       Heart Disease  Multivariate   \n",
       "2                                               Adult  Multivariate   \n",
       "3                                    Dry Bean Dataset  Multivariate   \n",
       "4                                            Diabetes                 \n",
       "..                                                ...           ...   \n",
       "13    Influenza outbreak event prediction via Twitter  Multivariate   \n",
       "14  Average Localization Error (ALE) in sensor nod...  Multivariate   \n",
       "15                              Turkish Music Emotion  Multivariate   \n",
       "16                           Gender Gap in Spanish WP  Multivariate   \n",
       "17                                             Raisin  Multivariate   \n",
       "\n",
       "              Task              Attribute type   No of instances  \\\n",
       "0   Classification                        Real     150 Instances   \n",
       "1   Classification  Categorical, Integer, Real     303 Instances   \n",
       "2   Classification        Categorical, Integer  48.84K Instances   \n",
       "3   Classification               Integer, Real  13.61K Instances   \n",
       "4                         Categorical, Integer                     \n",
       "..             ...                         ...               ...   \n",
       "13  Classification               Real, Integer  75.84K Instances   \n",
       "14      Regression                        Real     107 Instances   \n",
       "15  Classification               Real, Integer     400 Instances   \n",
       "16  Classification               Real, Integer   4.75K Instances   \n",
       "17  Classification               Real, Integer     900 Instances   \n",
       "\n",
       "   No of attribute Date Donated  \n",
       "0     4 Attributes    6/30/1988  \n",
       "1    13 Attributes    6/30/1988  \n",
       "2    14 Attributes    4/30/1996  \n",
       "3    16 Attributes    9/13/2020  \n",
       "4    20 Attributes          N/A  \n",
       "..             ...          ...  \n",
       "13  523 Attributes    8/14/2023  \n",
       "14    6 Attributes    8/13/2023  \n",
       "15   50 Attributes    8/14/2023  \n",
       "16   21 Attributes    8/13/2023  \n",
       "17    8 Attributes    8/13/2023  \n",
       "\n",
       "[643 rows x 7 columns]"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining lambda functions to retrieved each element\n",
    "fn_text = lambda web: text_el(web)\n",
    "attributes = {'Dataset name': [By.XPATH, '//h2[@class=\"truncate text-primary\"]', fn_text, False], \n",
    "             'Data type': [By.XPATH, '//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]/div[2]/span', fn_text, False],\n",
    "             'Task': [By.XPATH, '//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]/div[1]/span', fn_text, False],\n",
    "             'Attribute type': [By.XPATH, '//table/tbody/tr/td[2]', fn_text, False],\n",
    "             'No of instances': [By.XPATH, '//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]/div[3]/span', fn_text, False],\n",
    "             'No of attribute': [By.XPATH, '//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]/div[4]/span', fn_text, False],\n",
    "             'Date Donated': [By.XPATH, '//table/tbody/tr/td[3]', fn_text, False]}\n",
    "\n",
    "# Initializing the dataframe and setting n_pag\n",
    "df_final = pd.DataFrame({})\n",
    "n_pag = round(total_ds/25)\n",
    "\n",
    "# Scraping the datasets page by page\n",
    "for i in range(n_pag):\n",
    "    \n",
    "    time.sleep(10) #waiting time for page\n",
    "    df = load_items_by(driver, None, **attributes)\n",
    "    \n",
    "    # Checking n records retrieved\n",
    "    if len(df)>0:\n",
    "        df_final = pd.concat([df_final, df], axis=0)\n",
    "        print(f'{len(df_final)}/{total_ds} datasets retrieved')\n",
    "    else:\n",
    "        print(f\"0 datasets retrieved.\")\n",
    "        \n",
    "    # Going to next page\n",
    "    try:\n",
    "        if(i+2 <= n_pag and len(df_final) < total_ds and len(df)>0):\n",
    "            xpath_nextbtn = '//*[@aria-label=\"Next Page\"]'\n",
    "            go_next_page(driver, xpath_nextbtn)\n",
    "            print(f\"page: {i+2}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Something happend retrieving the data: {e.msg}\")\n",
    "        \n",
    "# Cleaning the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "49b05203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data frame\n",
    "display(df_final)\n",
    "df_final['No of attribute'] = df_final['No of attribute'].str.extract(r'(\\d+) Attributes$')\n",
    "df_final['No of instances'] = df_final['No of instances'].str.extract(r'(\\d\\w+) Instances$')\n",
    "df_final['Year'] = df_final['Date Donated'].str.extract(r'/(\\d+)$')\n",
    "df_final.fillna('-', inplace=True)\n",
    "\n",
    "# Saving the data frame\n",
    "df_final.to_csv('datasets/q8_UCI_datasets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "5250e0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset name</th>\n",
       "      <th>Data type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Attribute type</th>\n",
       "      <th>No of instances</th>\n",
       "      <th>No of attribute</th>\n",
       "      <th>Date Donated</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>150</td>\n",
       "      <td>4</td>\n",
       "      <td>6/30/1988</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heart Disease</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>303</td>\n",
       "      <td>13</td>\n",
       "      <td>6/30/1988</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>84K</td>\n",
       "      <td>14</td>\n",
       "      <td>4/30/1996</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dry Bean Dataset</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>61K</td>\n",
       "      <td>16</td>\n",
       "      <td>9/13/2020</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diabetes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>-</td>\n",
       "      <td>20</td>\n",
       "      <td>N/A</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Influenza outbreak event prediction via Twitter</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real, Integer</td>\n",
       "      <td>84K</td>\n",
       "      <td>523</td>\n",
       "      <td>8/14/2023</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Average Localization Error (ALE) in sensor nod...</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>107</td>\n",
       "      <td>6</td>\n",
       "      <td>8/13/2023</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Turkish Music Emotion</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real, Integer</td>\n",
       "      <td>400</td>\n",
       "      <td>50</td>\n",
       "      <td>8/14/2023</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Gender Gap in Spanish WP</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real, Integer</td>\n",
       "      <td>75K</td>\n",
       "      <td>21</td>\n",
       "      <td>8/13/2023</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Raisin</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real, Integer</td>\n",
       "      <td>900</td>\n",
       "      <td>8</td>\n",
       "      <td>8/13/2023</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>643 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Dataset name     Data type  \\\n",
       "0                                                Iris  Multivariate   \n",
       "1                                       Heart Disease  Multivariate   \n",
       "2                                               Adult  Multivariate   \n",
       "3                                    Dry Bean Dataset  Multivariate   \n",
       "4                                            Diabetes                 \n",
       "..                                                ...           ...   \n",
       "13    Influenza outbreak event prediction via Twitter  Multivariate   \n",
       "14  Average Localization Error (ALE) in sensor nod...  Multivariate   \n",
       "15                              Turkish Music Emotion  Multivariate   \n",
       "16                           Gender Gap in Spanish WP  Multivariate   \n",
       "17                                             Raisin  Multivariate   \n",
       "\n",
       "              Task              Attribute type No of instances  \\\n",
       "0   Classification                        Real             150   \n",
       "1   Classification  Categorical, Integer, Real             303   \n",
       "2   Classification        Categorical, Integer             84K   \n",
       "3   Classification               Integer, Real             61K   \n",
       "4                         Categorical, Integer               -   \n",
       "..             ...                         ...             ...   \n",
       "13  Classification               Real, Integer             84K   \n",
       "14      Regression                        Real             107   \n",
       "15  Classification               Real, Integer             400   \n",
       "16  Classification               Real, Integer             75K   \n",
       "17  Classification               Real, Integer             900   \n",
       "\n",
       "   No of attribute Date Donated  Year  \n",
       "0                4    6/30/1988  1988  \n",
       "1               13    6/30/1988  1988  \n",
       "2               14    4/30/1996  1996  \n",
       "3               16    9/13/2020  2020  \n",
       "4               20          N/A     -  \n",
       "..             ...          ...   ...  \n",
       "13             523    8/14/2023  2023  \n",
       "14               6    8/13/2023  2023  \n",
       "15              50    8/14/2023  2023  \n",
       "16              21    8/13/2023  2023  \n",
       "17               8    8/13/2023  2023  \n",
       "\n",
       "[643 rows x 8 columns]"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fe44ce",
   "metadata": {},
   "source": [
    "### Common functions\n",
    "All functions used are my own. As we have progressed in the assignments, I have been improving the code so that they are more general and can be applied in different scenarios. I decided to do it this way to improve and progress my knowledge of python.\n",
    "By Monica Atiaga "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4c7afef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attribute_el(webelement, attribute):\n",
    "    return (webelement.get_attribute(attribute))\n",
    "\n",
    "def text_el(webelement):\n",
    "    return webelement.text\n",
    "\n",
    "def go_next_page(driver, xpath):\n",
    "    ''' Go to the next page'''\n",
    "        \n",
    "    next_btn = driver.find_element(By.XPATH, xpath)\n",
    "    print(f\" Going to {next_btn.text}\")\n",
    "    \n",
    "    # scroll until button be visible\n",
    "    actions = ActionChains(driver)\n",
    "    actions.move_to_element(next_btn).perform()\n",
    "\n",
    "    # Wait until the button be visible (loading compleated)\n",
    "    wait = WebDriverWait(driver, 15)\n",
    "    wait.until(EC.element_to_be_clickable((By.XPATH, xpath)))\n",
    "\n",
    "    # Click the button once it is clickable\n",
    "    next_btn.click()\n",
    "\n",
    "def load_items_by(driver, n=None, **kwargs):\n",
    "    ''' Load items on the page with the attributes sent in kwargs.\n",
    "        n: number of products required\n",
    "        kwargs: dictionary with key, xpath_config(by, xpath, fn, is_list) \n",
    "        Return: a DataFrame with the info\n",
    "    '''\n",
    "    data_retrieved = {}\n",
    "    lenght_ref = -1\n",
    "    \n",
    "    for key, xpath_config in kwargs.items():\n",
    "        \n",
    "        by, xpath, fn, is_list = xpath_config\n",
    "        \n",
    "        try:\n",
    "            tags = driver.find_elements(by, xpath )  # scraping\n",
    "        \n",
    "            if fn is not None:\n",
    "                items = [ fn(tag) for tag in tags] # If lambda is specified it applies\n",
    "            else:\n",
    "                items = [ tag.text for tag in tags]  # Default: text function\n",
    "            \n",
    "            if(lenght_ref <0): # Setting the n items in the first iteration\n",
    "                lenght_ref = len(items)\n",
    "           \n",
    "            if(lenght_ref == len(items)):\n",
    "                data_retrieved[key] = items\n",
    "                print(f\"{key} retrieved and included:\", len(items))\n",
    "            else:\n",
    "                data_retrieved[key] = [None for _ in range(lenght_ref)]\n",
    "                print(f\"{key} retrieved NOT COMPATIBLE:\", len(items))\n",
    "        \n",
    "        except NoSuchElementException:\n",
    "            print(f\"{xpath} not found\")\n",
    "        \n",
    "    df = pd.DataFrame(data_retrieved)\n",
    "        \n",
    "    if ( bool(n) and (n < len(df))): #checking the n registers needed\n",
    "        return (df.iloc[0:n,:])\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def remove_popup(xpath):\n",
    "    # Check if an element on the new page is present\n",
    "    try:\n",
    "        close_button  = driver.find_element(By.XPATH, xpath)\n",
    "        close_button.click() \n",
    "        print(\"Pop-up removed\")\n",
    "    except NoSuchElementException:\n",
    "        print(\"No pop-up windows was found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "17f3754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_item_detail(driver, not_found_value = '-', **kwargs):\n",
    "    '''Retrieve all the attributes sent in kwargs.\n",
    "       Return: a Data frame with the data.'''\n",
    "    data = {}\n",
    "    \n",
    "    for key, xpath_config in kwargs.items():\n",
    "        items = []\n",
    "        \n",
    "        by, xpath_param, fn, is_list = xpath_config\n",
    "\n",
    "        try:\n",
    "            tag_item = driver.find_elements(by, xpath_param )\n",
    "            \n",
    "            if (not(is_list) and len(tag_item) > 0): \n",
    "                # adding attribute individually -Description \n",
    "                items.append(fn(tag_item[0]))\n",
    "            elif (is_list and len(tag_item) > 0): \n",
    "                # adding attributes as a list\n",
    "                items.append([fn(tag) for tag in tag_item])\n",
    "            else:\n",
    "                # If there is not attribute '-' : not_found_param\n",
    "                items.append(not_found_value)\n",
    "        except NoSuchElementException:\n",
    "            print(\"Element not found\")\n",
    "            items.append(not_found_value)\n",
    "\n",
    "        data[key] = items\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def scrape_data_by_urls(urls, attributes_det):\n",
    "    ''' Scrape the data in attributes_det from each url in urls.\n",
    "    urls: a Serie with URLs\n",
    "    attributes_det: attributes configuration to scrape the data'''\n",
    "    # Initializing dataframe\n",
    "    df_det = pd.DataFrame(columns=[*attributes_det.keys()])\n",
    "\n",
    "    for i, url in enumerate(urls):\n",
    "        \n",
    "        try:\n",
    "            driver.set_page_load_timeout(10) \n",
    "            driver.get(url)\n",
    "            print(f\"Retrieving Details of record: {i+1}/{len(urls)}, url: {url}\")\n",
    "        except TimeoutException:\n",
    "            print(f\"The page could not be loaded completed within the specified time. {i+1}/{len(urls)}, URL: {url}\")\n",
    "\n",
    "        wait_time = 3\n",
    "        time.sleep(wait_time)\n",
    "\n",
    "        # Scrape the data in each url\n",
    "        df2 = scrape_item_detail(driver, **attributes_det)\n",
    "        df_det = pd.concat([df_det, df2], ignore_index=True)\n",
    "            \n",
    "    return df_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "88c2a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scraping_table(xpath_table, init_data=1, footer=False):\n",
    "    ''' Recover the data from the table with xpath_table sent.\n",
    "    footer: Specifies if it is necessary to remove the footer of the table'''\n",
    "    table = driver.find_element(By.XPATH, xpath_table)\n",
    "\n",
    "    rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "\n",
    "    header_tags = rows[0].find_elements(By.TAG_NAME, 'th')\n",
    "    columns = [h.text for h in header_tags ]\n",
    "    print(f\"Columns: {columns}\")\n",
    "\n",
    "    table_data = []\n",
    "    \n",
    "    if footer:\n",
    "        end_row = len(rows) -1\n",
    "    else:\n",
    "        end_row = len(rows)\n",
    "\n",
    "    for row in rows[init_data:end_row]:\n",
    "        row_data = [cell.text for cell in row.find_elements(By.TAG_NAME, 'td')]\n",
    "        table_data.append(row_data)\n",
    "\n",
    "    print(f\"Recovered {len(table_data)} records.\")\n",
    "\n",
    "    df = pd.DataFrame(table_data)\n",
    "    \n",
    "    if len(columns) == df.shape[1] : \n",
    "        df.columns = columns\n",
    "        \n",
    "    return [df, columns]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
